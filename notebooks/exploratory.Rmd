---
title: "GAS simulations: GAS score distributions"
subtitle: "Exploratory analysis"
author: "Taylor Dunn"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: TRUE
    toc_float: TRUE
    theme: paper
    highlight: zenburn
bibliography: ../references.bib
---
<style>
body .main-container {
  max-width: 1500px !important;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.width = 7, fig.height = 4, dpi = 150
)
```

```{r packages, include=FALSE}
library(tidyverse)
library(here)
library(ardea)
library(targets)
library(gt)
library(glue)

extrafont::loadfonts(device = "win", quiet = TRUE)
set_ardea_defaults()
```

# Motivation

The simulation technique introduced by @Urach2019 returns latent variable goal scores which (in the absence of a treatment effect) will be normally distributed with mean 0 and SD 1:

```{r fig.height=3, fig.width=4}
d <- tibble(x_continuous = rnorm(1e4, mean = 0, sd = 1))

p <- d %>%
  ggplot(aes(x_continuous)) +
  geom_density(fill = "lightgrey") +
  labs(x = "Latent goal scores")
p
```

Latent goal scores are then discretized into the 5-point scales according to a set of thresholds.
The thresholds proposed by @Urach2019 were equally spaced along along the inverse cumulative normal distribution $\Phi^{-1}$:

$$
c_t = \Phi^{-1} (0.2t), \ \ \ \ \  t = 0, \dots, 5
$$

Here is how those thresholds look overlaid on the latent variable scale:

```{r fig.height=3, fig.width=4}
thresh_unif <- qnorm(seq(0, 1, 0.2), mean = 0, sd = 1)

p +
  geom_vline(xintercept = thresh_unif, lty = 2)
```

And this is the resulting distribution of discrete goal scores:

```{r fig.height = 3, fig.width=4}
d <- d %>%
  mutate(
    x_discrete_unif = gasr::discretize_from_thresholds(x_continuous,
                                                       thresh_unif),
    x_discrete_unif = factor(x_discrete_unif, levels = -2:2,
                             labels = c("-2", "-1", "0", "+1", "+2"))
  )
d %>%
  count(x_discrete_unif) %>%
  mutate(p = n / sum(n)) %>%
  ggplot(aes(x = x_discrete_unif, y = n)) +
  geom_col(fill = "lightgrey", color = "black") +
  geom_text(aes(label = scales::percent(p, accuracy = 0.1)), vjust = 0) +
  labs("Discrete goal scores (uniform thresholds)")
```

Theses thresholds, on the other hand:

```{r fig.height=3, fig.width=4}
thresh_norm <- gasr::create_thresholds()
p +
  geom_vline(xintercept = thresh_norm, lty = 2)
```

Which produce an approximately normal distribution of 5-point scores:

```{r fig.height = 3, fig.width=4}
d <- d %>%
  mutate(
    x_discrete_norm = gasr::discretize_from_thresholds(x_continuous,
                                                       thresh_norm),
    x_discrete_norm = factor(x_discrete_norm, levels = -2:2,
                             labels = c("-2", "-1", "0", "+1", "+2"))
  )
d %>%
  count(x_discrete_norm) %>%
  mutate(p = n / sum(n)) %>%
  ggplot(aes(x = x_discrete_norm, y = n)) +
  geom_col(fill = "lightgrey", color = "black") +
  geom_text(aes(label = scales::percent(p, accuracy = 0.1)), vjust = 0) +
  labs("Discrete goal scores (normal thresholds)")
```

How does the difference in score distributions affect the power to detect a treatment effect, and the estimated size of that treatment effect?

# Simulation methodology

To investigate this question, we will use the `gasr` package, available [here](https://github.com/taylordunn/gasr), which uses the method proposed by @Urach2019.
In brief, GAS data will be simulated as follows:

```{r}
library(gasr)

# Set the inter-correlation between goals
rho_0 <- 0.3
# And define the variances in terms of the goal inter-correlation
#  such that sigma_u^2 + sigma_e^2 = 1
sigma_u <- sqrt(rho_0)
sigma_e <- sqrt(1 - rho_0)

d <- sim_trial(
  n_subjects = 40, n_goals = 3, delta = 0.3, 
  sigma_u = sigma_u, sigma_e = sigma_e,
  score_dist = "norm" # the default argument
)
```

By default, `score_discrete` will be derived with normal thresholds (`score_dist` = "norm").
We can change the argument to `score_dist` = "unif", but it is simpler to use the same simulated `score_continuous` and apply uniform thresholds manually:

```{r fig.height=8}
d <- d %>%
  rename(score_discrete_norm = score_discrete) %>%
  mutate(
    score_discrete_unif = discretize_from_thresholds(
      score_continuous, thresh_unif
    )
  )
d_long <- d %>%
  select(subject_id, group, score_continuous,
         score_discrete_norm, score_discrete_unif) %>%
  pivot_longer(cols = starts_with("score_discrete"),
               names_to = "distribution", values_to = "score_discrete") %>%
  mutate(distribution = str_remove(distribution, "score_discrete_"))
thresh_plot <- tibble(
  distribution = "norm", score_continuous = thresh_norm
) %>%
  bind_rows(
    tibble(distribution = "unif", score_continuous = thresh_unif)
  ) %>%
  mutate(
    # Plot the infinity thresholds at +/- 4
    score_continuous = case_when(score_continuous < -4 ~ -4,
                                 score_continuous > 4 ~ 4,
                                 TRUE ~ score_continuous) 
  )

p1 <- d_long %>%
  ggplot(aes(x = score_continuous)) +
  geom_density(aes(fill = group), alpha = 0.5) +
  facet_grid(group ~ distribution) +
  geom_vline(
    data = thresh_plot, aes(xintercept = score_continuous), lty = 2
  ) +
  theme(legend.position = "none")
p2 <- d_long %>%
  ggplot(aes(x = factor(score_discrete))) +
  geom_bar(aes(fill = group), alpha = 0.5) +
  facet_grid(group ~ distribution) +
  theme(legend.position = "none")
library(patchwork)
p1 / p2
```

This will done for different numbers of subjects, numbers of goals, and treatment effect size.
Each set of parameters will be simulated 10,000 times each.

# Analysis

Load the power calculation results:

```{r}
tar_load(sim_data_tscore_ttest)
tar_load(sim_data_tscore_ttest_power)
```

The following simulations were performed:

```{r}
sim_data_tscore_ttest_power %>%
  select(-power, -score_dist) %>%
  distinct() %>%
  rename_with(~str_remove(., "sim_")) %>%
  gt() %>%
  tab_options(container.height = 500, container.overflow.y = TRUE)
```

## Power

Pre-process:

```{r}
d_power <- sim_data_tscore_ttest_power %>%
  rename_with(~str_remove(., "sim_")) %>%
  # Add some labels for plotting
  mutate(
    n_subjects_delta = glue("{n_subjects} subjects, delta = {delta}"),
    n_subjects_n_goals = glue("{n_subjects} subjects, ",
                              "{n_goals} goals per subject"),
    n_goals_label = glue("{n_goals} goals per subject"),
    power_label = scales::percent(power, accuracy = 1),
    score_dist_label = factor(score_dist, levels = c("norm", "unif"),
                              labels = c("Normal", "Uniform"))
  )
```

Visualize the power for 40 and 80 subjects:

```{r}
d_power %>%
  filter(n_subjects == 40) %>%
  ggplot(aes(x = delta, y = power, color = score_dist_label)) +
  geom_line() +
  geom_point() +
  facet_wrap(~n_goals_label) +
  scale_x_continuous("Treatment effect size",
                     breaks = c(0.3, 0.5, 0.7, 0.9, 1.1)) +
  scale_y_continuous("Power", labels = scales::percent) +
  labs(color = "Score distribution",
       title = "Power vs treatment effect for 40 subjects") +
  theme(legend.position = c(0.7, 0.2))
```

```{r}
d_power %>%
  filter(n_subjects == 80) %>%
  ggplot(aes(x = delta, y = power, color = score_dist_label)) +
  geom_line() +
  geom_point() +
  facet_wrap(~n_goals_label) +
  scale_x_continuous("Treatment effect size",
                     breaks = c(0.3, 0.5, 0.7, 0.9, 1.1)) +
  scale_y_continuous("Power", labels = scales::percent) +
  labs(color = "Score distribution",
       title = "Power vs treatment effect for 80 subjects") +
  theme(legend.position = c(0.7, 0.2))
```

What were the parameter combinations with the largest power differences?

```{r}
d_power %>%
  select(n_subjects, delta, n_goals, score_dist, power) %>%
  pivot_wider(names_from = score_dist, values_from = power) %>%
  rename(power_norm = norm, power_unif = unif) %>%
  mutate(
    power_diff = power_norm - power_unif
  ) %>%
  arrange(desc(abs(power_diff))) %>%
  mutate(across(c(power_norm, power_unif, power_diff),
                ~scales::percent(., accuracy = 0.1))) %>%
  gt() %>%
  tab_options(container.height = 300,
              container.overflow.y = TRUE)
```

Plot power difference for each parameter combination:

```{r fig.width=12, fig.height=8}
d_power %>%
  select(n_subjects, delta, n_goals, score_dist, power,
         n_subjects_delta, n_subjects_n_goals, n_goals_label) %>%
  pivot_wider(names_from = score_dist, values_from = power) %>%
  rename(power_norm = norm, power_unif = unif) %>%
  mutate(
    power_diff = power_norm - power_unif,
    power_diff_label = glue("{scales::percent(power_diff, accuracy = 0.1)}",
                            "\n({scales::percent(power_norm, accuracy = 0.1)})")
  ) %>%
  ggplot(aes(x = factor(delta), y = factor(n_subjects))) +
  geom_tile(aes(fill = power_diff)) +
  geom_text(aes(label = power_diff_label), color = "white") +
  facet_wrap(~n_goals_label) +
  theme(legend.position = "none") +
  scale_x_discrete("Treatment effect size", expand = c(0, 0)) +
  scale_y_discrete("Number of subjects", expand = c(0, 0)) +
  labs(title = "Difference in power, normal - uniform scores")
```

## Effect size

Compute effect size mean and SE:

```{r}
d_effect_size <- sim_data_tscore_ttest %>%
  rename_with(~str_remove(., "sim_")) %>%
  filter(stat %in% c("cohen_d", "tscore_diff",
                     "tscore_diff_conf_low", "tscore_diff_conf_high")) %>%
  group_by(n_subjects, n_goals, delta, rho, score_dist, stat) %>%
  summarise(
    n_sim = n(),
    mean_value = mean(value), sd_value = sd(value),
    .groups = "drop"
  ) %>%
  mutate(
    n_subjects_label = glue("{n_subjects} subjects"),
    delta_label = glue("delta = {delta}"),
    n_subjects_delta = glue("{n_subjects} subjects, delta = {delta}"),
    n_subjects_n_goals = glue("{n_subjects} subjects, ",
                              "{n_goals} goals per subject"),
    n_goals_label = glue("{n_goals} goals per subject"),
    score_dist_label = factor(score_dist, levels = c("norm", "unif"),
                              labels = c("Normal", "Uniform"))
  )
```

The difference in mean T-scores for 60 subjects:

```{r}
d_effect_size %>%
  filter(stat == "tscore_diff", n_subjects == 60) %>%
  # Reverse the sign on the score difference so that a positive value
  #  indicates higher scores in the treatment arm
  mutate(mean_value = -mean_value) %>%
  ggplot(aes(x = delta, y = mean_value, color = score_dist_label)) +
  geom_point() +
  geom_line() +
  facet_wrap(~n_goals_label) +
  scale_x_continuous("Treatment effect size",
                     breaks = c(0.3, 0.5, 0.7, 0.9, 1.1)) +
  scale_y_continuous("Mean difference in T-scores") +
  labs(color = "Score distribution",
       title = "Effect size vs treatment effect for 60 subjects") +
  theme(legend.position = c(0.7, 0.2))
```

In every case, the uniform distribution of scores results in a higher difference in T-score.
Now look at standardized effect sizes (Cohen's $d$):

$$
d = \frac{\mu_{\text{treatment}} - \mu_{\text{control}}}{\sigma_{\text{pooled}}}
$$

```{r}
d_effect_size %>%
  filter(stat == "cohen_d", n_subjects == 60) %>%
  ggplot(aes(x = delta, y = mean_value, color = score_dist_label)) +
  geom_point() +
  geom_line() +
  facet_wrap(~n_goals_label) +
  scale_x_continuous("Treatment effect size",
                     breaks = c(0.3, 0.5, 0.7, 0.9, 1.1)) +
  scale_y_continuous("Mean standardized effect size") +
  labs(color = "Score distribution",
       title = "Effect size vs treatment effect for 60 subjects") +
  theme(legend.position = c(0.7, 0.2))
```

Look at 3 goals per subject, and different numbers of subjects:

```{r}
d_effect_size %>%
  filter(stat == "cohen_d", n_goals == 3) %>%
  ggplot(aes(x = delta, y = mean_value, color = score_dist_label)) +
  geom_point() +
  geom_line() +
  facet_wrap(~n_subjects_label) +
  scale_x_continuous("Treatment effect size",
                     breaks = c(0.3, 0.5, 0.7, 0.9, 1.1)) +
  scale_y_continuous("Mean standardized effect size") +
  labs(color = "Score distribution",
       title = "Effect size vs treatment effect for 3 goals per subject") +
  theme(legend.position = c(0.7, 0.2))
```

What were the parameter combinations with the largest effect size differences?

```{r}
d_effect_size %>%
  filter(stat == "cohen_d") %>%
  select(n_subjects, delta, n_goals, score_dist, mean_value, sd_value) %>%
  pivot_wider(names_from = score_dist,
              values_from = c(mean_value, sd_value)) %>%
  mutate(
    effect_size_diff = mean_value_norm - mean_value_unif,
    effect_size_norm = glue(
      "{round(mean_value_norm, 2)} ({round(sd_value_norm, 2)})"
    ),
    effect_size_unif = glue(
      "{round(mean_value_unif, 2)} ({round(sd_value_unif, 2)})"
    )
  ) %>%
  arrange(desc(abs(effect_size_diff))) %>%
  mutate(effect_size_diff = round(effect_size_diff, 2)) %>%
  select(-mean_value_norm, -mean_value_unif, -sd_value_norm, -sd_value_unif) %>%
  gt() %>%
  tab_options(container.height = 300,
              container.overflow.y = TRUE)
```

It is clear from this ranking that the score distributions make little difference in the measured effect size.
The largest differences are at the largest effect sizes.
Take, for example, 40 subjects, 3 goals per subject, and $\delta$ = 1.1:

```{r fig.height=2}
sim_data_tscore_ttest %>%
  filter(sim_n_subjects == 40, sim_n_goals == 3, sim_delta == 1.1,
         stat == "cohen_d") %>%
  mutate(
    score_dist_label = factor(score_dist, levels = c("norm", "unif"),
                              labels = c("Normal", "Uniform"))
  ) %>%
  ggplot(aes(y = score_dist_label, x = value)) +
  geom_jitter(aes(color = score_dist), alpha = 0.05) +
  geom_boxplot(fill = "white", outlier.shape = NA, width = 0.2) +
  theme(legend.position = "none") +
  ardea::theme_ardea(gridlines = "x") +
  labs(x = "Standardized effect size", y = NULL) +
  theme(legend.position = "none")
```

Another way to think of the difference is to consider the percentage of simulations in which the effect size is higher with normal scores vs uniform scores:

```{r}
sim_data_tscore_ttest %>%
  filter(stat == "cohen_d") %>%
  pivot_wider(names_from = score_dist, values_from = value) %>%
  group_by(across(starts_with("sim_"))) %>%
  summarise(
    n_sims = n(), p_higher = mean(norm > unif),
    .groups = "drop"
  ) %>%
  arrange(desc(p_higher)) %>%
  transmute(
    n_subjects = sim_n_subjects, n_goals = sim_n_goals, delta = sim_delta,
    n_sims, p_higher = scales::percent(p_higher, accuracy = 0.1)
  ) %>%
  gt() %>%
  tab_options(container.height = 300, container.overflow.y = TRUE)
```

At the lower end of the spectrum (i.e. the lowest effect sizes), it is essentially even: in ~51% of simulations, the normal scores return a higher effect size than uniform.

Plot effect size difference for each parameter combination:

```{r fig.width=12, fig.height=8}
d_effect_size %>%
  filter(stat == "cohen_d") %>%
  select(n_subjects, delta, n_goals, score_dist, mean_value,
         n_subjects_delta, n_subjects_n_goals, n_goals_label) %>%
  pivot_wider(names_from = score_dist, values_from = mean_value) %>%
  rename(effect_size_norm = norm, effect_size_unif = unif) %>%
  mutate(
    effect_size_diff = effect_size_norm - effect_size_unif,
    effect_size_diff_label = glue(
      "{round(effect_size_diff, 2)}",
      "\n({round(effect_size_norm, 2)})"
    )
  ) %>%
  ggplot(aes(x = factor(delta), y = factor(n_subjects))) +
  geom_tile(aes(fill = effect_size_diff)) +
  geom_text(aes(label = effect_size_diff_label), color = "white") +
  facet_wrap(~n_goals_label) +
  theme(legend.position = "none") +
  scale_x_discrete("Treatment effect size", expand = c(0, 0)) +
  scale_y_discrete("Number of subjects", expand = c(0, 0)) +
  labs(title = "Difference in effect size, normal - uniform")
```

Number of subjects makes little difference, and neither does goals per subject.


# Reproducibility

<details><summary>Reproducibility receipt</summary>

```{r}
Sys.time()
```

```{r}
if ("git2r" %in% installed.packages()) {
  if (git2r::in_repository()) {
    git2r::repository()
  }
}
```

```{r}
sessioninfo::session_info()
```

</details>

# References
